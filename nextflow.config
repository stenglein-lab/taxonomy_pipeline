params {
  // -------------------------
  // Input/Output directories
  // -------------------------
  input_dir    = "input"
  fastq_dir    = "${params.input_dir}/fastq/"
  outdir       = "results"

  // pattern to match for fastq files
  fastq_pattern         = "*_R[12]*.fastq*"
}

// a test profile
profiles {
  test {
    includeConfig 'conf/test.config'
  }
}

params {

  initial_fastqc_dir    = "${params.outdir}/initial_fastqc/"
  post_trim_fastqc_dir  = "${params.outdir}/post_trim_fastqc/"
  host_filtered_out_dir = "${params.outdir}/host_filtered_fastq/"
  remapping_out_dir     = "${params.outdir}/remapped/"
  contigs_out_dir       = "${params.outdir}/contigs/"
  blast_out_dir         = "${params.outdir}/blast_output/"
  tally_out_dir         = "${params.outdir}/tallies/"
  virus_seq_out_dir     = "${params.outdir}/virus_sequences/"
  virus_remap_out_dir   = "${params.outdir}/virus_remapping/"
  counts_out_dir        = "${params.outdir}/fastq_counts/"
  fastq_out_dir         = "${params.outdir}/trimmed_fastq/"
  bam_out_dir           = "${params.outdir}/bam/"
  tax_db_out_dir        = "${params.outdir}/taxonomy_databases/"
  virus_matrix_out_dir  = "${params.outdir}/taxa_matrices/"

  // an option to subset input datasets
  // set to the subset fraction (0.1: 10% of reads will be subsampled)
  subsample_fraction    = null

  // reports on running the pipeline itself
  tracedir = "${params.outdir}/pipeline_info"

  // where are R and shell scripts are found.
  R_bindir         = "${baseDir}/scripts"
  scripts_bindir   = "${baseDir}/scripts"

  // ------------------
  // Trimming settings 
  // ------------------
  always_trim_5p_bases = "0"
  always_trim_3p_bases = "1"
  // if you have very short library molecules may want to change this
  post_trim_min_length = "30"

  // cd-hit-dup cutoff for collapsing reads with >= this much fractional similarity
  mismatches_allowed = "2"

  // --------------------
  // Host cell filtering 
  // --------------------
  // Define one of the 2 following parameters:
  //
  // 1. A 2-column tab-delimited file with:
  //    - the first column defining dataset IDs or patterns that will
  //      match dataset IDs
  //    - the second column will be the path of a bowtie index that will be 
  //      used to filter out host reads
  //
  //    This enables different filtering for different datasets                   
  host_map_file = null
                                                                                
  // 2. The path to a bowtie index that will be used to filter host reads
  //    for all datasets
  // 
  // params.host_bt_index = "/home/databases/fly/combined_fly_index"
  host_bt_index = null
                                                                                
  // min bowtie alignment score to be considered a host-derived read
  host_bt_min_score = "40"

  // -------------------------
  // BLAST and classification
  // -------------------------
                                                                                
  // minimum length of contigs to keep for further analysis
  minimum_contig_length = 200

  // classify singletons (reads that don't map to contigs) in addition to just contigs?
  // classifying singletons is slower but more thorough
  classify_singletons = false
                                                                                
  // Blast e-value cutoffs
  max_blast_nt_evalue = "1e-10"
  max_blasx_nr_evalue = "1e-3"

  // the directory containing the local NCBI nt blast database
  local_nt_database_dir       = "/home/databases/nr_nt/"
  // the name of the local copy of the NCBI nt blast database: nt by default
  local_nt_database_name      = "nt"

  // the directory containing the local diamond nr database
  local_diamond_database_dir  = "/home/databases/nr_nt/"
  // the name of the local copy of the Diamond nr database
  local_diamond_database_name = "nr.dmnd"

  // a directory containing a local copy of the blast taxonomy information
  // from: https://ftp.ncbi.nlm.nih.gov/blast/db/taxdb.tar.gz
  // if this is set to null, the pipeline will download a new copy in the
  // working directory
  blast_tax_dir              = null
  blast_tax_url              = "https://ftp.ncbi.nlm.nih.gov/blast/db/taxdb.tar.gz"

  // a directory containing NCBI taxonomy database
  ncbi_tax_dir               = null

  // name of NCBI Taxonomy SQLite database
  // created using script download_and_process_taxonomy_databases
  ncbi_tax_db                = "ncbi_taxonomy.sqlite3"

  // the minimum number of reads mapping to a virus to be output in the virus-mapping
  // read matrix
  min_matrix_reads           = 5


  // singularity_pull_docker_container option
  //
  // turn this parameter on to pull docker containers and convert to singularity
  //
  // see e.g.: https://nf-co.re/gwas#quick-start, which states:
  //
  //   "If you are persistently observing issues downloading Singularity images directly
  //    due to timeout or network issues then please use the --singularity_pull_docker_container
  //    parameter to pull and convert the Docker image instead."
  //
  // TODO: this option is provided in nf-core pipelines but is it necessary?
  //       possibly remove this option and the corresponding if/else statment in processes?
  //

  singularity_pull_docker_container = false

}

process {

  // ------------------------------------------------------------
  // setup resource usage limits for different types of processes
  // ------------------------------------------------------------

  // high memory process like blastn (using nt database)
  withLabel: 'highmem' {
    maxForks = 2
    cpus = 24
  }

  // low memory processes that use multi-threading
  // like bowtie2
  withLabel: 'lowmem_threaded' {
    maxForks = 6
    cpus = 8
  }

  // low memory processes that don't use multi-threading
  withLabel: 'lowmem_non_threaded' {
    maxForks = 24
    cpus = 1
  }
}

/*
   Profiles allow you to run on different servers or with different base configurations

   See: https://www.nextflow.io/docs/latest/config.html#config-profiles
*/
profiles {

  local {
    exector.name = 'local'
    executor.queueSize = 64
    executor.cpus = 64
    executor.memory = '384 GB'
    // if the pipeline has to access system paths outside of $HOME, $PWD, etc 
    // have to bind those paths to singularity.
    // see: https://sylabs.io/guides/latest/user-guide/bind_paths_and_mounts.html
    // in this profile, we are pointing to local intallations of NCBI databases 
    //so need to access those paths
    singularity.runOptions = "--bind /home/databases"
    params.local_nt_database ="/home/databases/nr_nt/nt"
    params.local_diamond_database ="/home/databases/nr_nt/nr.dmnd"
    params.remote_blast = false
  }

  conda {
    params.enable_conda    = true
    process.conda          = "./conda/taxonomy_conda_environment.yaml"
    singularity.enabled    = false
    conda.cacheDir         = "$HOME/conda_cacheDir"
  }

  singularity {
    params.enable_conda    = false
    singularity.enabled    = true
    singularity.autoMounts = true
    singularity.cacheDir   = "$HOME/singularity_cacheDir"
    // singularity.runOptions = "-B /home/databases"
  }
}

def trace_timestamp = new java.util.Date().format( 'yyyy-MM-dd_HH-mm-ss')
timeline {
    enabled = true
    file    = "${params.tracedir}/execution_timeline_${trace_timestamp}.html"
}
report {
    enabled = true
    file    = "${params.tracedir}/execution_report_${trace_timestamp}.html"
}
trace {
    enabled = true
    file    = "${params.tracedir}/execution_trace_${trace_timestamp}.txt"
}
dag {
    enabled = true
    file    = "${params.tracedir}/pipeline_dag_${trace_timestamp}.pdf"
}

manifest {
    name            = 'stenglein-lab/taxonomy'
    author          = 'Mark Stenglein'
    homePage        = 'https://github.com/stenglein-lab/taxonomy'
    description     = 'A pipeline to taxonomically classify sequences from Illumina datasets'
    mainScript      = 'main.nf'
    nextflowVersion = '!>=21.04.0'
    version         = '1.0'
}


// Turn this option on to delete all intermediate files from the analysis
// see: https://www.nextflow.io/docs/latest/config.html
// cleanup = true
